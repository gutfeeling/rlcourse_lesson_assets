{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import `gym` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To setup a Reinforcement Learning problem in `gym`, call `gym.make()` with the name of the problem\n",
    "- Returns an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To initialize the problem, call `env.reset()`\n",
    "- Returns the initial observation of the Agent once the environment is initialized.\n",
    "- 1st number: cart position (initialized to nearly the center)\n",
    "- 2nd number: cart velocity (initialized to nearly zero; hardly moving)\n",
    "- 3rd number pole angle with the vertical (initialized to nearly vertical)\n",
    "- 4rth number: pole velocity at tip (initialized to nearly zero; hardly swinging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00870804 0.04742115 0.02188616 0.00178741]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To take an action, call `env.step()` with the action as argument\n",
    "- The first element returned by the `env.step()` function is the new environment state.\n",
    "- The second element returned by the `env.step()` is called the reward. The reward is a judgement of the environment state and the action at any time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.00965647 -0.14800773  0.02192191  0.30129455]\n",
      "reward is 1.0\n"
     ]
    }
   ],
   "source": [
    "observation, reward, _, _ = env.step(0)\n",
    "print(\"observation is {}\".format(observation))\n",
    "print(\"reward is {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The learning goal (balancing the pole without crashing) is equivalent to maximizing the reward function over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking multiple actions in a Python loop. Call `env.render()` after each action to update the environment state and visualize the dynamics in real time\n",
    "- Use `time.sleep()` to get a slow motion version.\n",
    "- Check that reward is `0` when pole angle is greater than 12 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9133875525328697 1.0\n",
      "-0.8808171123059392 1.0\n",
      "-0.5186689201043438 1.0\n",
      "0.17326848975626852 1.0\n",
      "1.197312747080096 1.0\n",
      "2.557834489342066 1.0\n",
      "4.261167382640635 1.0\n",
      "6.315474910404569 1.0\n",
      "8.730564390075116 1.0\n",
      "11.517636185214313 1.0\n",
      "14.68895474570218 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibya/dropbox_venvs/rl_mooc_contents_python3.8/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.25742873307078 0.0\n",
      "22.236091135214007 0.0\n",
      "26.63747810638965 0.0\n",
      "31.472918137193552 0.0\n",
      "36.75176089290628 0.0\n",
      "42.48059550909299 0.0\n",
      "48.66252634536548 0.0\n",
      "55.29658240747022 0.0\n",
      "62.377325950823874 0.0\n",
      "69.89468979503067 0.0\n",
      "77.8340123689323 0.0\n",
      "86.1761654132462 0.0\n",
      "94.89760163824336 0.0\n",
      "103.97011236952248 0.0\n",
      "113.3601002312336 0.0\n",
      "123.02725711559171 0.0\n",
      "132.92270843671957 0.0\n",
      "142.98694976548154 0.0\n",
      "153.14823921091192 0.0\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "import time\n",
    "observation = env.reset()\n",
    "for _ in range(30):\n",
    "    pole_angle_in_radians = observation[2]\n",
    "    # 360 degrees is equal to 2 * pi radians\n",
    "    pole_angle_in_degrees = (pole_angle_in_radians * 360) / (2 * pi)\n",
    "    observation, reward, _, _ = env.step(0)\n",
    "    print(pole_angle_in_degrees, reward)\n",
    "    env.render()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To close the visual representation, call `env.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
