{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import `gym` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To setup a Reinforcement Learning problem in `gym`, call `gym.make()` with the name of the problem\n",
    "- Returns an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To initialize the problem, call `env.reset()`\n",
    "- Returns the initial observation of the Agent once the environment is initialized.\n",
    "- 1st number: cart position (initialized to nearly the center)\n",
    "- 2nd number: cart velocity (initialized to nearly zero; hardly moving)\n",
    "- 3rd number pole angle with the vertical (initialized to nearly vertical)\n",
    "- 4rth number: pole velocity at tip (initialized to nearly zero; hardly swinging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03619636  0.03696681 -0.02245762 -0.04815029]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To take an action, call `env.step()` with the action as argument\n",
    "- The first element returned by the `env.step()` function is the new environment state.\n",
    "- The second element returned by the `env.step()` is called the reward. The reward is a judgement of the environment state and the action at any time step.\n",
    "- The third element is called `done`. It denotes if the `observation` is a terminal state. It is `bool`. \n",
    "    - It is `True` if it a terminal state \n",
    "    - `False` if not a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation is [ 0.0369357  -0.15782604 -0.02342063  0.23736332]\n",
      "reward is 1.0\n",
      "done is False\n"
     ]
    }
   ],
   "source": [
    "observation, reward, done, _ = env.step(0)\n",
    "print(\"observation is {}\".format(observation))\n",
    "print(\"reward is {}\".format(reward))\n",
    "print(\"done is {}\".format(done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some problems are inherently \"episodic\"\n",
    "- Terminal states: states after which taking actions are not allowed.\n",
    "- Episode: the period between the first observation (`env.reset()`) to the terminal state(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal states in CartPole-v0\n",
    "1. Terminal state 1: when the pole angle becomes 12 degrees for the first time (\"Game over\")\n",
    "2. Terminal state 2: when the cart touches the X axis bounds for the first time (\"Game over\")\n",
    "3. Terminal state 3: comes after 200 time steps (\"You win the game\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The learning goal (balancing the pole without crashing for 200 time steps) is equivalent to maximizing the reward function over an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In any episodic RL problem, the first step is to express the goal as the maximization of some reward function over an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking actions for many episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "num_episodes = 5\n",
    "for _ in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        observation, reward, done, _ = env.step(0)\n",
    "        env.render()\n",
    "        # time.sleep(0.1)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To close the visual representation, call `env.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
