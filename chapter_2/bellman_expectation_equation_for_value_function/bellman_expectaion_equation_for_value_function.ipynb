{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman expectation equation for value functions\n",
    "\n",
    "$V_{\\pi}(s(t)) = \\mathbf{E}[r(t) + \\gamma V_{\\pi}(s(t+1)) | s(t)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying the Bellman expectation equation in `CartPole-v0`\n",
    "\n",
    "- Use the initial state `[0., 0.01, 0.15, 0.]` i.e. the pole tilted to the right at initialization.\n",
    "- Use the epsilon pole direction policy with epsilon 0.9.\n",
    "- Use gamma 0.8.\n",
    "- Verify the Bellman expectation equation for the initial state `[0., 0.01, 0.15, 0.]` by relating it to the value functions of the next possible states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the state (0.0, 0.01, 0.15, 0.0), given the epsilon pole direction policy with epsilon 0.9 is 4.580812449061228\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    \"\"\"Wrapper class to change initial state  in CartPole-v0\n",
    "    \"\"\"\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "    \n",
    "\n",
    "def get_action_random(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "    \n",
    "    \n",
    "def get_action_epsilon_pole_direction_policy(observation):\n",
    "    \"\"\"Sampling function for the epsilon pole direction policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.9:\n",
    "        return get_action_random(observation)\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "class Value():\n",
    "    def __init__(self, gamma, visit_number={}, value_average={}):\n",
    "        \"\"\"\n",
    "        Helper for computing expected value of states. \n",
    "        It has an update() method that updates averages of value samples with new episode data\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.visit_number = visit_number\n",
    "        self.value_average = value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        backward_reward_sum = 0\n",
    "        for step in reversed(episode_history):\n",
    "            backward_reward_sum = self.gamma * backward_reward_sum + step[\"reward\"]\n",
    "            key = tuple(step[\"observation\"])\n",
    "            try:\n",
    "                visit_number = self.visit_number[key]\n",
    "            except KeyError:\n",
    "                visit_number = 0\n",
    "            if visit_number == 0:\n",
    "                self.value_average[key] = backward_reward_sum\n",
    "            else:\n",
    "                self.value_average[key] = (visit_number * self.value_average[key] + backward_reward_sum) / (visit_number + 1)\n",
    "            self.visit_number[key] = visit_number + 1\n",
    "\n",
    "            \n",
    "# create the wrapped env where the pole it tilted to the right in the initial state    \n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))\n",
    "\n",
    "\n",
    "# compute expected value of states by going through 100000 episodes\n",
    "num_episodes = 100000\n",
    "gamma = 0.8\n",
    "\n",
    "value_info = Value(gamma=gamma)\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = pole_right_init_cartpole_env.reset()\n",
    "    while True:\n",
    "        action = get_action_epsilon_pole_direction_policy(observation)\n",
    "        next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    value_info.update(episode_history)\n",
    "pole_right_init_cartpole_env.close()\n",
    "\n",
    "state = (0., 0.01, 0.15, 0.)\n",
    "print(f\"The value of the state {state}, given the epsilon pole direction policy with epsilon 0.9 is {value_info.value_average[state]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.00000000e-04 -1.86919275e-01  1.50000000e-01  3.35996937e-01]\n"
     ]
    }
   ],
   "source": [
    "initial_observation = pole_right_init_cartpole_env.reset()\n",
    "next_observation_action_0, reward, done, _ = pole_right_init_cartpole_env.step(0)    # prob: 0.45\n",
    "print(next_observation_action_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.00000000e-04  2.02687997e-01  1.50000000e-01 -2.41851667e-01]\n"
     ]
    }
   ],
   "source": [
    "initial_observation = pole_right_init_cartpole_env.reset()\n",
    "next_observation_action_1, reward, done, _ = pole_right_init_cartpole_env.step(1)    # prob: 0.55\n",
    "print(next_observation_action_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman expectation equation for value functions\n",
    "\n",
    "$V_{\\pi}(s(t)) = \\mathbf{E}[r(t) + \\gamma V_{\\pi}(s(t+1)) | s(t)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.580674388322798"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + gamma * (value_info.value_average[tuple(next_observation_action_0)] * 0.45 + value_info.value_average[tuple(next_observation_action_1)] * 0.55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
