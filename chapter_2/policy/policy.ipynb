{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last lesson \n",
    "\n",
    "- The agent exerts a lot of control over how an episode will unfold by *choosing* the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A particular way of choosing actions, given an environment state, is called a POLICY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation):\n",
    "    \"\"\"\n",
    "    :param observation: A gym observation of an environment state\n",
    "    :return: The probability of taking different actions in that environment state e.g. {0: 0.8, 1: 0.2}\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(observation):\n",
    "    return {0: 0.5, 1: 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `env.step()` demands a concrete action sampled from the prob. distr. given by the policy. So we need a sampling function for every policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())    # this demands a concrete action\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the random policy\n",
    "import random \n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A function that runs a given number of episodes while the agent is following a particular policy and returns the average total rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "\n",
    "def get_average_total_rewards_per_episode(policy_sampling_function, num_episodes):\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    total_rewards = 0\n",
    "    for num_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            if num_episode == 0:\n",
    "                env.render()\n",
    "            action = policy_sampling_function(observation)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return total_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.177"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(get_action_random_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Pole direction policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pole_direction_policy(observation):\n",
    "    if observation[2] > 0: \n",
    "        return {0: 0, 1: 1}\n",
    "    return {0: 1, 1: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "def get_action_pole_direction_policy(observation):\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.335"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(get_action_pole_direction_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different policies lead to different total rewards per episode\n",
    "\n",
    "- \"Pole direction policy\" ~ 40\n",
    "- \"random policy\" ~ 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which policy would give me the most total rewards per episode?\n",
    "\n",
    "## Central goal of the agent in any RL problem: find (or learn) the *policy* that maximizes total rewards per episode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
