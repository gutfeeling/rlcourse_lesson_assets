{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function in terms of discounted reward sum: goodness of a state given a certain policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are vegetarian\n",
    "\n",
    "- Action choices: \n",
    "    1. eating plant based food\n",
    "    2. eat animal based food \n",
    "    \n",
    "- Given any state $s$, you choose action number 1.\n",
    "- $\\pi(1 | s) = 1, \\pi(2 | s) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You wanted to change the policy (maybe out of curiosity), but not by too much. What is the smallest change that you can make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For one step (once in your life), and one step only, you are going to try animal based food. \n",
    "- For every time step after that, you are going to continue following your vegetarian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You were following a policy $\\pi$ in a MDP, the smallest change to the policy is the following\n",
    "- You take a particular action $a$ for only a single time step (ignore the $\\pi(a | s)$ in that particular time step).\n",
    "- Then afterwards, you continue following policy $\\pi$ for all subsequent time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The action value function (which depends on that action $a$ that you took in that single time step and the state $s$ that you were in) measures the result of this slight deviation from the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the random policy\n",
    "import random \n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are interested in computing the action value function sample for state `[0, 0.01, 0.15, 0]` and the action `1`: `5.69`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6953279000000006\n"
     ]
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "step_count = 0\n",
    "discounted_reward_sum = 0\n",
    "gamma = 0.9\n",
    "while True:\n",
    "    if step_count == 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = get_action_random_policy(observation)\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    observation = next_observation\n",
    "    discounted_reward_sum += reward * (gamma**step_count)\n",
    "    step_count += 1\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()\n",
    "print(discounted_reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are interested in computing the action value function sample for state `[0, 0.01, 0.15, 0]` and the action `0`: `4.68`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.68559\n"
     ]
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "step_count = 0\n",
    "discounted_reward_sum = 0\n",
    "gamma = 0.9\n",
    "while True:\n",
    "    if step_count == 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = get_action_random_policy(observation)\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    observation = next_observation\n",
    "    discounted_reward_sum += reward * (gamma**step_count)\n",
    "    step_count += 1\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()\n",
    "print(discounted_reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State (s) | Action (a) | Policy ($\\pi$) | Sample of $Q_{\\pi}(s, a)$ from one episode |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | 1 | random | `5.69` |\n",
    "| `[0, 0.01, 0.15, 0]` | 0 | random |  `4.68` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValue:\n",
    "    def __init__(self, gamma, visit_number=None, q_value_average=None):\n",
    "        \"\"\"\n",
    "        visit_number: {(observation, action): 3}\n",
    "        q_value_average: {(observation, action): 4.5}\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        if visit_number is None:\n",
    "            self.visit_number = {}\n",
    "        else:\n",
    "            self.visit_number = visit_number\n",
    "        if q_value_average is None:\n",
    "            self.q_value_average = {}\n",
    "        else:\n",
    "            self.q_value_average = q_value_average\n",
    "        \n",
    "    def update(self, episode_history):\n",
    "        backward_reward_sum = 0\n",
    "        for step in reversed(episode_history):\n",
    "            backward_reward_sum = (self.gamma * backward_reward_sum) + step[\"reward\"]\n",
    "            key = (tuple(step[\"observation\"]), step[\"action\"])\n",
    "            try:\n",
    "                visit_number = self.visit_number[key]\n",
    "            except KeyError:\n",
    "                visit_number = 0\n",
    "            if visit_number == 0:\n",
    "                self.q_value_average[key] = backward_reward_sum\n",
    "            else:\n",
    "                self.q_value_average[key] = ((visit_number * self.q_value_average[key]) + backward_reward_sum) / (visit_number + 1)\n",
    "            self.visit_number[key] = visit_number + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_info = QValue(gamma=0.9)\n",
    "num_episodes = 1000\n",
    "for num_episode in range(num_episodes):\n",
    "    episode_history = []\n",
    "    observation = pole_right_init_cartpole_env.reset()\n",
    "    while True:\n",
    "        action = get_action_random_policy(observation)\n",
    "        next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "        episode_history.append({\"observation\": observation, \"reward\": reward, \"action\": action})\n",
    "        observation = next_observation\n",
    "        if done:\n",
    "            break\n",
    "    q_value_info.update(episode_history)       \n",
    "pole_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.157367163378453\n",
      "477\n"
     ]
    }
   ],
   "source": [
    "state_action_pair = ((0, 0.01, 0.15, 0), 1)\n",
    "print(q_value_info.q_value_average[state_action_pair])\n",
    "print(q_value_info.visit_number[state_action_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.339425112972492\n",
      "523\n"
     ]
    }
   ],
   "source": [
    "state_action_pair = ((0, 0.01, 0.15, 0), 0)\n",
    "print(q_value_info.q_value_average[state_action_pair])\n",
    "print(q_value_info.visit_number[state_action_pair])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State (s) | Action (a) | Policy ($\\pi$) | $Q_{\\pi}(s, a)$ |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | 1 | random | 8.15 |\n",
    "| `[0, 0.01, 0.15, 0]` | 0 | random | 6.33 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
