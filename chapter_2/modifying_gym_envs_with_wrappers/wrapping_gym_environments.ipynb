{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of model free RL: Irrespective of the details of the MDP, find (or learn) the policy that maximizes total rewards per episode\n",
    "- Since an episode starts with the initial state and ends with the terminal state, the total rewards in an episode depends on the *initial state*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gym` makes a choice for the initialization of the `CartPole-v0` environment, but this is not the only choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03272082  0.00775598  0.04386464  0.03129368]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "print(observation)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we made a different choice of initialization in `CartPole-v0`, how would that affect the total rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we at all change the default initialization?\n",
    "- Yes! `gym` offers a way to modify details of any environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a `gym` environment?\n",
    "- Must have the following attributes\n",
    "    - `observation_space`\n",
    "    - `action_space`\n",
    "    - ...\n",
    "- Must have certain methods\n",
    "    - `reset()`\n",
    "    - `step()`\n",
    "    - `render()`\n",
    "    - `close()`\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In `gym`, the way to modify an existing environment is to create a *wrapped environment* from the existing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestWrappedEnv():    # SimplestWrappedEnv is wrapper. SimplestWrappedEnv() is a wrapped environment.\n",
    "    # wrapped environment must have a reference to existing environment\n",
    "    def __init__(self, env):    # env is the existing environment\n",
    "        self.env = env\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        return self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gym.Wrapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWrappedEnv(gym.Wrapper):    # WHen we inherit from gym.Wrapper, it copies all attrs and methods of the existing environment automatically\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_v0_env = gym.make(\"CartPole-v0\")\n",
    "my_wrapped_env = MyWrappedEnv(env=cartpole_v0_env)    # wrapped environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_wrapped_env.env == cartpole_v0_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_wrapped_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_wrapped_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01328838 -0.02252086  0.00251714  0.04043948]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = my_wrapped_env.reset()\n",
    "print(observation)\n",
    "my_wrapped_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, _ = my_wrapped_env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wrapped_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A wrapper that modifies the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleReward(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        doubled_reward = reward * 2\n",
    "        return observation, doubled_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "double_reward_cartpole_env = DoubleReward(env=cartpole_v0_env)\n",
    "observation = double_reward_cartpole_env.reset()\n",
    "observation, reward, done, _ = double_reward_cartpole_env.step(0)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_reward_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A big advantage of wrappers is that they can be applied to any `gym` environment\n",
    "- You can use `DoubleReward` wrapper on both `CartPole-v0` and `MountainCar-v0`.\n",
    "- No need to do it twice, which would be the case if we were overriding methods e.g. `step()` of each environment individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_reward_mountaincar_env = DoubleReward(env=gym.make(\"MountainCar-v0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more general version of a reward scaling wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardScaling(gym.Wrapper):    # Always inherit from gym.Wrapper\n",
    "    def __init__(self, env, scaling_factor):\n",
    "        super().__init__(env)     # super important to call super().__init__(env) if we are overriding the init function\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)    # super important to call self.env.step()\n",
    "        reward *= self.scaling_factor\n",
    "        return observation, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_reward_cartpole_env = RewardScaling(env=cartpole_v0_env, scaling_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A  wrapped environment can be further wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "original_reward_cartpole_env = RewardScaling(env=double_reward_cartpole_env, scaling_factor=0.5)\n",
    "observation = original_reward_cartpole_env.reset()\n",
    "observation, reward, done, _ = original_reward_cartpole_env.step(1)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_reward_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CartPole-v0` is itself a wrapped environment!\n",
    "- The base environment is `gym.envs.classic_control.cartpole.CartPoleEnv` (actually defines the dynamics)\n",
    "    - Has everything in `CartPole-v0` except for the terminal state at 200 time steps.\n",
    "- `TimeLimit` wrapper is applied on the base environment `gym.wrappers.time_limit.TimeLimit`. \n",
    "    - This introduces the terminal state at 200 time steps.\n",
    "    \n",
    "`CartPole-v0 = TimeLimit(env=CartPoleEnv, max_episode_steps=200)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "print(cartpole_v0_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RewardScaling<RewardScaling<TimeLimit<CartPoleEnv<CartPole-v0>>>>>\n"
     ]
    }
   ],
   "source": [
    "print(original_reward_cartpole_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the initialization of the `CartPole-v0` env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization choice: pole to the right \n",
    "\n",
    "1. Cart position = 0\n",
    "2. Cart velocity = 0.01\n",
    "3. Pole angle = 0.15\n",
    "4. The pole tip velocity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pole_right_init_cartpole_env = InitMod(env=cartpole_v0_env, initial_state=np.array([0, 0.01, 0.15, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify that the wrapper does what it is supposed to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.15 0.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "print(observation)\n",
    "pole_right_init_cartpole_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_right_init_cartpole_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
