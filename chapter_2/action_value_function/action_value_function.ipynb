{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function in terms of discounted reward sum: goodness of a state given a certain policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You are vegetarian\n",
    "\n",
    "- Action choices: \n",
    "    1. eating plant based food\n",
    "    2. eat animal based food \n",
    "    \n",
    "- Given any state $s$, you choose action number 1.\n",
    "- $\\pi(1 | s) = 1, \\pi(2 | s) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You wanted to change the policy (maybe out of curiosity), but not by too much. What is the smallest change that you can make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For one step (once in your life), and one step only, you are going to try animal based food. \n",
    "- For every time step after that, you are going to continue following your vegetarian policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You were following a policy $\\pi$ in a MDP, the smallest change to the policy is the following\n",
    "- You take a particular action $a$ for only a single time step (ignore the $\\pi(a | s)$ in that particular time step).\n",
    "- Then afterwards, you continue following policy $\\pi$ for all subsequent time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The action value function (which depends on that action $a$ that you took in that single time step and the state $s$ that you were in) measures the result of this slight deviation from the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the random policy\n",
    "import random \n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are interested in computing the action value function sample for state `[0, 0.01, 0.15, 0]` and the action `1`: `5.69`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6953279000000006\n"
     ]
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "step_count = 0\n",
    "discounted_reward_sum = 0\n",
    "gamma = 0.9\n",
    "while True:\n",
    "    if step_count == 0:\n",
    "        action = 1\n",
    "    else:\n",
    "        action = get_action_random_policy(observation)\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    observation = next_observation\n",
    "    discounted_reward_sum += reward * (gamma**step_count)\n",
    "    step_count += 1\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()\n",
    "print(discounted_reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are interested in computing the action value function sample for state `[0, 0.01, 0.15, 0]` and the action `0`: `4.68`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.68559\n"
     ]
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "step_count = 0\n",
    "discounted_reward_sum = 0\n",
    "gamma = 0.9\n",
    "while True:\n",
    "    if step_count == 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = get_action_random_policy(observation)\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    observation = next_observation\n",
    "    discounted_reward_sum += reward * (gamma**step_count)\n",
    "    step_count += 1\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()\n",
    "print(discounted_reward_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State (s) | Action (a) | Policy ($\\pi$) | Sample of $Q_{\\pi}(s, a)$ from one episode |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | 1 | random | `5.69` |\n",
    "| `[0, 0.01, 0.15, 0]` | 0 | random |  `4.68` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
