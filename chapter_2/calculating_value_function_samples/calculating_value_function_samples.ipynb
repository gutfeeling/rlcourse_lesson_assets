{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last lesson\n",
    "\n",
    "- A wrapper for `CartPole-v0` that lets us choose the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization choice: pole to the right \n",
    "\n",
    "1. Cart position = 0\n",
    "2. Cart velocity = 0.01\n",
    "3. Pole angle = 0.15\n",
    "4. The pole tip velocity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.01 0.15 0.  ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = pole_right_init_cartpole_env.reset()\n",
    "print(observation)\n",
    "pole_right_init_cartpole_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does changing the initial state have an impact on the total rewards?\n",
    "- For the default initialization, we get an average total reward of around 22.5 when following the random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_total_rewards_per_episode(env, policy_sampling_function, num_episodes):\n",
    "    total_rewards = 0\n",
    "    for num_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            if num_episode == 0:\n",
    "                env.render()\n",
    "            action = policy_sampling_function(observation)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    print(f\"Average total rewards per episode for {num_episodes} episodes is {total_rewards / num_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling function for the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the random policy\n",
    "import random \n",
    "\n",
    "def get_action_random_policy(observation):\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total rewards per episode for 1000 episodes is 16.746\n"
     ]
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization choice: non zero pole velocity\n",
    "\n",
    "1. Cart position = 0\n",
    "2. Cart velocity = 0.01\n",
    "3. Pole angle = 0.\n",
    "4. The pole tip velocity = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "non_zero_pole_velocity_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0., 2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average reward for the \"non zero pole velocity\" initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total rewards per episode for 1000 episodes is 6.102\n"
     ]
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(non_zero_pole_velocity_init_cartpole_env, get_action_random_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The expected total rewards per episode, while following a policy, depends on the starting state!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value of a state, given a policy: The expected total rewards per episode you get starting from the state if you follow the given policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State | State description | Policy | Value function |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | Pole angled to the right | random | 16 |\n",
    "| `[0., 0.01, 0., 2.0]` | Non zero pole tip velocity | random | 6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling function for the epsilon pole direction policy with $\\epsilon=0.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_action_epsilon_pole_direction_policy(observation):\n",
    "    if random.random() < 0.9:\n",
    "        return get_action_random_policy(observation)\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average reward for \"pole right\" init and epsilon pole direction policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total rewards per episode for 1000 episodes is 19.728\n"
     ]
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_epsilon_pole_direction_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average reward for \"non zero pole velocity\" init and epsilon pole direction policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total rewards per episode for 1000 episodes is 6.346\n"
     ]
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(non_zero_pole_velocity_init_cartpole_env, get_action_epsilon_pole_direction_policy, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State | State description | Policy | Value function |\n",
    "| --- | --- | --- | --- |\n",
    "| `[0, 0.01, 0.15, 0]` | Pole angled to the right | random | 16 |\n",
    "| `[0, 0.01, 0.15, 0]` | Pole angled to the right | epsilon pole direction | 19 |\n",
    "| `[0., 0.01, 0., 2.0]` | Non zero pole tip velocity | random | 6 |\n",
    "| `[0., 0.01, 0., 2.0]` | Non zero pole tip velocity | epsilon pole direction | 6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three important points about the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because of the *memoryless property of MDP*, the value of state depends *only on the state and the policy*. It doesn't matter if we encounter the state in the first time step (initialization) or we encounter it in the 50th step. The average total rewards that you get afterwards while following a policy remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is clear from the examples that some states have more value than others, given a policy. From the agent's poin t of view, the value function is an indication of the \"goodness\" of a state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The value function is an expected value. Since the policy may have probabilistic components, and MDPs may have non-trivial state transition probabilities, the total rewards that you get in different episodes starting from a state will be different. These values (from just one episode) are called *value function samples*. When you repeat over many epsiodes, the average of the samples start converging towards a certain number. This is the *expected value* and is the value function (value of a state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total rewards per episode for 1 episodes is 13.0\n",
      "Average total rewards per episode for 10 episodes is 15.8\n",
      "Average total rewards per episode for 100 episodes is 15.63\n",
      "Average total rewards per episode for 1000 episodes is 15.892\n",
      "Average total rewards per episode for 10000 episodes is 16.4143\n"
     ]
    }
   ],
   "source": [
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 1)\n",
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 10)\n",
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 100)\n",
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 1000)\n",
    "get_average_total_rewards_per_episode(pole_right_init_cartpole_env, get_action_random_policy, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating value samples of all states encountered in an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_history = []\n",
    "\n",
    "observation = pole_right_init_cartpole_env.reset()\n",
    "while True:\n",
    "    action = get_action_random_policy(observation)\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "    observation = next_observation\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'observation': array([0.  , 0.01, 0.15, 0.  ]), 'reward': 1.0},\n",
       " {'observation': array([ 2.00000000e-04,  2.02687997e-01,  1.50000000e-01, -2.41851667e-01]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.00425376, 0.00577724, 0.14516297, 0.09413264]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.0043693 , -0.19109477,  0.14704562,  0.42886288]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.00054741, 0.0016721 , 0.15562288, 0.1859088 ]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.00058085,  0.19426474,  0.15934105, -0.05392031]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.00446615,  0.3867858 ,  0.15826265, -0.29239534]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.01220186, 0.18980299, 0.15241474, 0.04572142]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.01599792, -0.00713863,  0.15332917,  0.38234589]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.01585515, 0.18551163, 0.16097609, 0.14166309]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.01956538, -0.0115058 ,  0.16380935,  0.48049128]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.01933527, 0.18097088, 0.17341917, 0.24358609]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.02295468, -0.01614916,  0.1782909 ,  0.58556118]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.0226317 , 0.1760866 , 0.19000212, 0.35391871]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.02615343, 0.36807077, 0.19708049, 0.12665004]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.03351485, 0.17075164, 0.19961349, 0.47446662]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([0.03692988, 0.36257795, 0.20910283, 0.25073813]),\n",
       "  'reward': 1.0}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_samples_random_policy = {}\n",
    "backward_reward_sum = 0\n",
    "for step in reversed(episode_history):\n",
    "    backward_reward_sum += step[\"reward\"]\n",
    "    value_samples_random_policy[tuple(step[\"observation\"])] = backward_reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.03692988041286534, 0.36257794775102353, 0.20910282630949142, 0.2507381259917327) 1.0\n",
      "(0.033514847600599786, 0.17075164061327763, 0.19961349397149078, 0.4744666169000328) 2.0\n",
      "(0.02615343217990403, 0.3680707710347878, 0.19708049310200385, 0.12665004347434686) 3.0\n",
      "(0.02263170024120346, 0.17608659693502837, 0.1900021189135659, 0.3539187094218975) 4.0\n",
      "(0.022954683494889327, -0.016149162684293283, 0.17829089540086557, 0.5855611756350163) 5.0\n",
      "(0.019335265948893687, 0.18097087729978212, 0.17341917356648776, 0.24358609171889067) 6.0\n",
      "(0.01956538185397864, -0.011505795254247803, 0.16380934789594478, 0.48049128352714854) 7.0\n",
      "(0.015855149340997812, 0.18551162564904142, 0.1609760861940366, 0.14166308509540823) 8.0\n",
      "(0.01599792199288572, -0.0071386325943954, 0.15332916839442762, 0.3823458899804495) 9.0\n",
      "(0.01220186214899767, 0.1898029921944025, 0.15241473990575496, 0.04572142443363275) 10.0\n",
      "(0.004466146217074472, 0.38678579659615986, 0.1582626467659552, -0.292395343010012) 11.0\n",
      "(0.0005808514361683836, 0.19426473904530442, 0.15934105303907378, -0.053920313655929586) 12.0\n",
      "(0.000547409343551799, 0.0016721046308292298, 0.15562287712048264, 0.18590879592955714) 13.0\n",
      "(0.0043693046527076675, -0.19109476545779341, 0.14704561952004364, 0.42886288002194983) 14.0\n",
      "(0.004253759939302361, 0.005777235670265335, 0.14516296666500866, 0.09413264275174879) 15.0\n",
      "(0.0002, 0.20268799696511808, 0.15, -0.2418516667495668) 16.0\n",
      "(0.0, 0.01, 0.15, 0.0) 17.0\n"
     ]
    }
   ],
   "source": [
    "for key, value in value_samples_random_policy.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
