{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use the discounted reward sum? Why not just add up the rewards normally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation 1: (psychology) Humans and other animals seems to instinctively apply a weighting factor to future rewards.\n",
    "- The future rewards are inherently uncertain.\n",
    "- The weighting factor is a way to factor in the risks associated with uncertain future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makes a lot of sense in the model free setting\n",
    "- The agent does not know the MDP (the state transition probs and the reward function in advance)\n",
    "- The value of a state, given a policy, is an expected value. The only way to find the value function is to go through the MDP infinite number of times (infinite number of episodes) (theoretically). \n",
    "- The agent feels that it can never get a true sense of the value function of a state.\n",
    "- TO compensate for that, it decides to calculate a different quanity that it feels will converge faster than the canonical value function (takes less samples to converge). \n",
    "- Easiest way to do that is to disocunt the contributions from the most uncertain parts (future rewards) of the value function calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation 2: Reward sum is not well defined in MDPs without terminal states, but the discounted reward sum is well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation 3: Some rare RL problems somehow naturally includes a discount factor (because of the nature of the rewards)\n",
    "\n",
    "- Financial markets.\n",
    "- Because of the effect of *interest rate*, dollar 10 right now might be better than dollar 10 after 4 years, because dollar 10 might grow to become dollar 11 or 12 because of accrueing interest. \n",
    "- *The interest rate* is a natural discount factor in problems related to financial markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "    \n",
    "import numpy as np\n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=np.array([0, 0.01, 0.15, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_history = []\n",
    "\n",
    "observation = pole_right_init_cartpole_env.reset()\n",
    "while True:\n",
    "    action = pole_right_init_cartpole_env.action_space.sample()\n",
    "    next_observation, reward, done, _ = pole_right_init_cartpole_env.step(action)\n",
    "    episode_history.append({\"observation\": observation, \"reward\": reward})\n",
    "    observation = next_observation\n",
    "    if done:\n",
    "        break\n",
    "pole_right_init_cartpole_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'observation': array([0.  , 0.01, 0.15, 0.  ]), 'reward': 1.0},\n",
       " {'observation': array([ 2.00000000e-04,  2.02687997e-01,  1.50000000e-01, -2.41851667e-01]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.00425376,  0.39538451,  0.14516297, -0.48371596]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.01216145,  0.58819155,  0.13548865, -0.72735653]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.02392528,  0.39148251,  0.12094152, -0.39528519]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.03175493,  0.19487074,  0.11303581, -0.06705158]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.03565235,  0.38820588,  0.11169478, -0.32204175]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.04341646,  0.58157475,  0.10525395, -0.57751759]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.05504796,  0.77507623,  0.09370359, -0.83527599]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.07054948,  0.96880178,  0.07699807, -1.09708095]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.08992552,  1.16283015,  0.05505646, -1.3646461 ]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.11318212,  0.96706355,  0.02776353, -1.05526273]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.13252339,  1.16180672,  0.00665828, -1.33910347]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.15575953,  1.3568442 , -0.02012379, -1.62969569]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.18289641,  1.55219675, -0.0527177 , -1.92858118]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.21394035,  1.74784239, -0.09128933, -2.23713377]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.24889719,  1.94370208, -0.136032  , -2.55650177]),\n",
       "  'reward': 1.0},\n",
       " {'observation': array([ 0.28777124,  2.1396226 , -0.18716204, -2.88753783]),\n",
       "  'reward': 1.0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_samples_random_policy = {}\n",
    "gamma = 0.9\n",
    "backward_reward_sum = 0\n",
    "for step in reversed(episode_history):\n",
    "    backward_reward_sum = (gamma * backward_reward_sum) + step[\"reward\"]\n",
    "    value_samples_random_policy[tuple(step[\"observation\"])] = backward_reward_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.28777123575678687, 2.1396226018779734, -0.18716203833204625, -2.8875378288098035) 1.0\n",
      "(0.24889719409596556, 1.943702083041065, -0.1360320029267657, -2.5565017702640267) 1.9\n",
      "(0.21394034626397745, 1.7478423915994057, -0.09128932759414701, -2.2371337666309348) 2.71\n",
      "(0.18289641130054746, 1.5521967481714996, -0.05271770404489147, -1.9285811774627768) 3.439\n",
      "(0.1557595273416303, 1.3568441979458585, -0.02012379027308613, -1.6296956885902667) 4.0951\n",
      "(0.1325233929872728, 1.1618067177178732, 0.006658279181731782, -1.3391034727408955) 4.68559\n",
      "(0.11318212191608223, 0.9670635535595291, 0.027763533835231263, -1.055262732674974) 5.217031\n",
      "(0.08992551900138579, 1.1628301457348222, 0.055056455782348086, -1.3646460973558412) 5.6953279000000006\n",
      "(0.07054948338639869, 0.968801780749355, 0.07699807474966608, -1.0970809483658996) 6.12579511\n",
      "(0.05504795870648505, 0.775076233995682, 0.09370359455527014, -0.835275990280203) 6.5132155990000005\n",
      "(0.04341646379355925, 0.5815747456462901, 0.10525394643344155, -0.5775175939085704) 6.861894039100001\n",
      "(0.03565234621774852, 0.38820587879053625, 0.11169478151366981, -0.32204175401141305) 7.175704635190001\n",
      "(0.03175493140289792, 0.19487074074253039, 0.11303581303794524, -0.06705157621377178) 7.458134171671\n",
      "(0.0239252811264161, 0.39148251382409094, 0.12094151691717517, -0.3952851939614966) 7.7123207545039\n",
      "(0.012161450095169706, 0.5881915515623197, 0.13548864744717723, -0.727356526500103) 7.94108867905351\n",
      "(0.004253759939302361, 0.39538450779336726, 0.14516296666500866, -0.4837159608915719) 8.14697981114816\n",
      "(0.0002, 0.20268799696511808, 0.15, -0.2418516667495668) 8.332281830033345\n",
      "(0.0, 0.01, 0.15, 0.0) 8.49905364703001\n"
     ]
    }
   ],
   "source": [
    "for key, value in value_samples_random_policy.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of model free RL: Irrespective of the MDP, find the policy that maximizes the expected discounted reward sum per episode (value function of the initial state)\n",
    "\n",
    "# Goal of model free RL: Irrespective of the MDP, find the policy that maximizes the expected discounted reward sum (value) of all states in the MDP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
