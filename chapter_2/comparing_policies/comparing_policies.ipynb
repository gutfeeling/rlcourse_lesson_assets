{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the goal in model free RL\n",
    "\n",
    "## Irrespective of the details of the MDP, find the *policy* that maximizes discounted reward sum (value) for all states in the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies we have seen\n",
    "\n",
    "- Random\n",
    "- Pole direction policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_action_random(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def get_action_pole_direction_policy(observation):\n",
    "    \"\"\"Sampling function for random policy\n",
    "    \"\"\"\n",
    "    if observation[2] > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitively, the pole direction policy seems better than a random policy. But is it really so?\n",
    "\n",
    "- Need to have a precise definition for how to compare different policies (**ordering** of policies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy $\\pi_{1}$ is *better* than $\\pi_{2}$ if <br><br>\n",
    "<center>$\\Large v_{\\pi_{1}}(s) \\geq v_{\\pi_{2}}(s), \\; \\forall s$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `CartPole-v0` has huge number of states\n",
    "\n",
    "- Several `100,000` states in `10000` episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "default_state = np.array([0., 0., 0., 0.])\n",
    "pole_right_state = np.array([0., 0.01, 0.15, 0.])\n",
    "pole_moving_state = np.array([0., 0., 0., 2.])\n",
    "cart_right_state = np.array([2.4, 0., 0., 0.])\n",
    "cart_moving_state = np.array([0., 10., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class InitMod(gym.Wrapper):\n",
    "    \"\"\"Wrapper class to change initial state  in CartPole-v0\n",
    "    \"\"\"\n",
    "    def __init__(self, env, initial_state):\n",
    "        super().__init__(env)\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        self.unwrapped.state = self.initial_state\n",
    "        return self.unwrapped.state\n",
    "\n",
    "    \n",
    "default_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=default_state)\n",
    "pole_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=pole_right_state)\n",
    "pole_moving_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=pole_moving_state)\n",
    "cart_right_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=cart_right_state)\n",
    "cart_moving_init_cartpole_env = InitMod(env=gym.make(\"CartPole-v0\"), initial_state=cart_moving_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_envs = [default_init_cartpole_env, \n",
    "                pole_right_init_cartpole_env, \n",
    "                pole_moving_init_cartpole_env, \n",
    "                cart_right_init_cartpole_env,\n",
    "                cart_moving_init_cartpole_env\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy for comparison\n",
    "\n",
    "- Set initial state using the wrapper.\n",
    "- Compute the average discounted reward sum obtained in an episode starting from that initial state (the value of the initial state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_for_initial_state(envs, policy_sampling_function, num_episodes, gamma):\n",
    "    for env in envs:\n",
    "        drs = 0\n",
    "        for num_episode in range(num_episodes):\n",
    "            observation = env.reset()\n",
    "            step_count = 0\n",
    "            while True:\n",
    "                if num_episode == 0:\n",
    "                    env.render()\n",
    "                action = policy_sampling_function(observation)\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                drs += reward * gamma ** step_count\n",
    "                step_count += 1\n",
    "                if done:\n",
    "                    break\n",
    "        env.close()\n",
    "        print(f\"Value for the initial state {env.initial_state} is {drs / num_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for the initial state [0. 0. 0. 0.] is 12.790677021304347\n",
      "Value for the initial state [0.   0.01 0.15 0.  ] is 9.977442194273785\n",
      "Value for the initial state [0. 0. 0. 2.] is 5.243647256552028\n",
      "Value for the initial state [2.4 0.  0.  0. ] is 5.751802573526081\n",
      "Value for the initial state [ 0. 10.  0.  0.] is 9.348804032817483\n"
     ]
    }
   ],
   "source": [
    "get_value_for_initial_state(wrapped_envs, get_action_random, 10000, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the pole direction policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for the initial state [0. 0. 0. 0.] is 18.868767452972946\n",
      "Value for the initial state [0.   0.01 0.15 0.  ] is 10.734175396802499\n",
      "Value for the initial state [0. 0. 0. 2.] is 5.298162187497666\n",
      "Value for the initial state [2.4 0.  0.  0. ] is 6.7315913742164515\n",
      "Value for the initial state [ 0. 10.  0.  0.] is 9.192798246744898\n"
     ]
    }
   ],
   "source": [
    "get_value_for_initial_state(wrapped_envs, get_action_pole_direction_policy, 10000, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| State (s) | $V_{\\textrm{random}}(s)$ | $V_{\\textrm{pole-direction}}(s)$ |\n",
    "| --- | --- | --- |\n",
    "| `[0, 0., 0., 0.]` | &emsp;&emsp;&emsp;&emsp;13.65 | &emsp;&emsp;&emsp;&emsp;&emsp;18.87 | \n",
    "| `[0, 0.01, 0.15, 0]` | 10.08 | 10.73 | \n",
    "| `[0. 0. 0. 2.]` | 5.27 | 5.30 | \n",
    "| `[2.4 0.  0.  0. ]` | 5.79 | 6.73 | \n",
    "| `[ 0. 10.  0.  0.]` | 9.35 | 9.19 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pole direction policy is not a better policy than the naive random policy. The random policy is also not better than the pole direction policy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
