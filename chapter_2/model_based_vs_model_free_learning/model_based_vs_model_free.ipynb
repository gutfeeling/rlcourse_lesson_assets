{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last lesson\n",
    "\n",
    "## Central goal of the agent in RL: Find (or learn) the *policy* that maximizes total rewards over an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Model based learning.\n",
    "\n",
    "- We have concrete knowledge of the MDP i.e. we know the values of $P_{ss^{'}}^a$ and $R_{ss^{'}}^a$ for any $s$, $s^{'}$ and $a$\n",
    "\n",
    "## Rewards in `CartPole-v0`\n",
    "\n",
    "| $s$ | $s^{'}$ | $a$ | $R_{ss^{'}}^a$ |\n",
    "| --- | --- | --- | --- |\n",
    "| non-terminal states | any | any | 1 |\n",
    "\n",
    "## State transition probabilities in `CartPole-v0`\n",
    "\n",
    "| $s$ | $s^{'}$ | $a$ | $P_{ss^{'}}^a$ |\n",
    "| --- | --- | --- | --- |\n",
    "| `[-0.01948913  0.03812474  0.04141217 -0.00638285]` | `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | 1 | 1 |\n",
    "| `[-0.01948913  0.03812474  0.04141217 -0.00638285]` | `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | 0 | 0 |\n",
    "| `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | `[-0.01407405  0.03694338  0.03557016  0.01969511]` | 1 | 0 |\n",
    "| `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | `[-0.01407405  0.03694338  0.03557016  0.01969511]` | 0 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "## Find the best policy exploiting this knowledge: model-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: Model free learning\n",
    "\n",
    "- Assume *NO* knowledge of the MDP i.e. we *DON'T* a-priori know the values of $P_{ss^{'}}^a$ and $R_{ss^{'}}^a$ for any $s$, $s^{'}$ and $a$\n",
    "\n",
    "| $s$ | $s^{'}$ | $a$ | $R_{ss^{'}}^a$ |\n",
    "| --- | --- | --- | --- |\n",
    "| non-terminal states | any | any | ? |\n",
    "\n",
    "| $s$ | $s^{'}$ | $a$ | $P_{ss^{'}}^a$ |\n",
    "| --- | --- | --- | --- |\n",
    "| `[-0.01948913  0.03812474  0.04141217 -0.00638285]` | `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | 1 | ? |\n",
    "| `[-0.01948913  0.03812474  0.04141217 -0.00638285]` | `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | 0 | ? |\n",
    "| `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | `[-0.01407405  0.03694338  0.03557016  0.01969511]` | 1 | ? |\n",
    "| `[-0.01872663  0.23262906  0.04128451 -0.2857175 ]` | `[-0.01407405  0.03694338  0.03557016  0.01969511]` | 0 | ? |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "## Learn the best policy without any prior knowledge of the MDP: model free learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage\n",
    "\n",
    "- It's going to work for any RL problem: e.g. both `CartPole-v0` and `MountainCar-v0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central goal: *Irrespective of the details of the MDP*, find (or learn) the *policy* that maximizes total rewards over an episode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
